🌳 결정트리 (Decision Tree) - 장점과 단점
📋 목차

결정트리란?
주요 특징
장점
단점
장단점 비교표
개선 방법
결론


결정트리란?
결정트리는 트리 구조를 이용하여 의사결정 규칙을 나타내는 예측 모델입니다.
              날씨가 맑나요?
                 /        \
              Yes          No
               |            |
          골프를 친다    실내 활동을 한다

내부 노드: 특성에 대한 테스트
가지: 테스트의 결과
잎 노드: 클래스 레이블


주요 특징

💡 핵심 포인트

사람의 의사결정 과정과 유사하여 직관적
분류(Classification)와 회귀(Regression) 모두 가능
복잡한 비선형 관계 모델링 가능



✅ 장점
1. 🎨 해석 가능성 (Interpretability)

트리 구조로 인해 모델의 의사결정 과정을 시각적으로 이해 가능
비전문가도 쉽게 해석할 수 있음
"왜 이런 결정을 했는지" 명확하게 설명 가능

2. 🚀 빠른 예측 속도

한 번 학습된 후에는 새로운 데이터에 대한 예측이 매우 빠름
O(log n) 시간 복잡도로 예측 수행
실시간 시스템에 적합

3. 🔧 전처리 과정 불필요

데이터 정규화나 스케일링 등의 전처리 과정이 필요하지 않음
원본 데이터를 그대로 사용 가능
데이터 준비 시간 단축

4. 🗂️ 혼합 데이터 타입 처리

수치형과 범주형 데이터를 동시에 처리 가능
별도의 인코딩 과정 불필요
다양한 형태의 데이터셋에 적용 가능

5. 🎯 자동 특성 선택

중요한 특성을 자동으로 선택
차원의 저주 문제를 어느 정도 해결
불필요한 특성은 자연스럽게 제외

6. 💪 누락값 처리

누락된 값이 있어도 비교적 잘 동작
대체 분할(Surrogate Split) 방법 제공
데이터 품질이 완벽하지 않아도 사용 가능


❌ 단점
1. 📊 과적합 경향 (Overfitting)

훈련 데이터에 너무 특화되어 일반화 성능 저하
복잡한 트리 구조로 인한 문제
새로운 데이터에 대한 예측 성능 떨어질 수 있음

2. ⚖️ 불균형 데이터에 취약

클래스 불균형이 심한 데이터에서 편향된 결과
다수 클래스에 치우친 예측
소수 클래스 예측 성능 저하

3. 🌊 모델 불안정성

작은 데이터 변화에도 완전히 다른 트리 생성 가능
훈련 데이터의 작은 변화에 민감
재현성 문제 발생 가능

4. 📏 특성 편향

범주가 많은 특성을 선호하는 경향
정보 이득 계산 시 편향 발생
일부 특성이 과도하게 선택될 수 있음

5. 📐 선형 관계 모델링의 어려움

단순한 선형 관계도 복잡한 트리로 표현해야 함
비효율적인 모델 구조
선형 모델이 더 적합한 경우에도 복잡하게 모델링

6. 🎲 확률 추정의 한계

클래스 확률을 정확히 추정하기 어려움
보정(Calibration)이 필요할 수 있음
불확실성 정보 제공에 한계


⚖️ 장단점 비교표
구분장점 ✅단점 ❌해석성매우 높음 (시각적 이해 가능)복잡한 트리는 해석 어려움성능 안정성빠른 예측 속도데이터 변화에 민감데이터 요구사항전처리 불필요, 혼합 데이터 처리클래스 불균형에 취약일반화비선형 패턴 학습 가능과적합 경향 높음특성 선택자동 특성 선택특성 편향 가능성노이즈 처리누락값 처리 가능노이즈에 민감

🔧 개선 방법
1. 🌲 앙상블 기법 (Ensemble Methods)
python# Random Forest 예시
from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators=100, random_state=42)

# Gradient Boosting 예시  
from sklearn.ensemble import GradientBoostingClassifier
gb = GradientBoostingClassifier(n_estimators=100, random_state=42)
2. ✂️ 가지치기 (Pruning)

Pre-pruning: 트리 성장 중 조기 중단

max_depth, min_samples_split, min_samples_leaf 등 설정


Post-pruning: 완성된 트리에서 불필요한 가지 제거

3. 📊 교차 검증 (Cross Validation)
pythonfrom sklearn.model_selection import cross_val_score
scores = cross_val_score(model, X, y, cv=5)
print(f"평균 정확도: {scores.mean():.3f} (+/- {scores.std() * 2:.3f})")
4. ⚖️ 클래스 불균형 해결

가중치 조정: class_weight='balanced'
샘플링 기법: SMOTE, 언더샘플링 등
비용 민감 학습: 오분류 비용 조정


🎯 결론
📈 적합한 상황

✅ 해석 가능성이 중요한 업무
✅ 빠른 프로토타이핑이 필요한 경우
✅ 혼합 데이터 타입을 다뤄야 하는 상황
✅ 도메인 전문가와의 협업이 중요한 프로젝트

📉 부적합한 상황

❌ 높은 예측 정확도가 최우선인 경우
❌ 선형 관계가 주를 이루는 데이터
❌ 클래스 불균형이 심각한 데이터
❌ 노이즈가 많은 데이터

💡 최종 권장사항

결정트리는 단독으로 사용하기보다는 앙상블 기법과 함께 사용할 때 그 진가를 발휘합니다.


탐색적 데이터 분석 단계에서 데이터 패턴 파악
기준선 모델로 활용하여 다른 모델과 비교
Random Forest, XGBoost 등 앙상블 기법의 기반 모델로 사용
해석 가능성이 중요한 최종 모델로 활용


📚 참고 자료

scikit-learn Decision Trees
Introduction to Statistical Learning
Hands-On Machine Learning
